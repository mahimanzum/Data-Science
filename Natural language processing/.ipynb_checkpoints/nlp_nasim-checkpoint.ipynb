{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"chadni posor posor rate rate jege jege aschi aschi ami ami shudhu shudhu  tOmake Ekobar chOcbob, taropor hob itihas\\\n",
    "amar shorIrer chaya ghurote ghurote chayahIn EkoTi rekhay Ese dacboRiyeche\\\n",
    "pagolI amar ghumiye poReche muThOfOn tai shant, ami rat jege diCchi pahara muThOfOner Ei prant\\\n",
    "amake diyeche jonm roktojhora omor Ekushe\\\n",
    "na jani se aj kOn rupe Eseche amar gacboye\"\n",
    "total_words = len(text.split(' '))\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.word_tokenize(text)\n",
    "unigrams = ngrams(token, 1)\n",
    "bigrams = ngrams(token,2)\n",
    "a = Counter(unigrams)\n",
    "b = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Count\n",
      "\n",
      "46\n",
      "\n",
      "Bigram Count\n",
      "\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print (\"Unigram Count\\n\")\n",
    "#for i in range(len(a)):\n",
    "    #print(a[i])\n",
    "\n",
    "print(len(a))\n",
    "print (\"\\nBigram Count\\n\")\n",
    "print (len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = list(Counter(a).items())\n",
    "b1 = list(Counter(b).items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chadni', 'posor')\n",
      "\"chadni\" \"posor\" 1\n",
      "\n",
      "('posor', 'posor')\n",
      "\"posor\" \"posor\" 1\n",
      "\n",
      "('posor', 'rate')\n",
      "\"posor\" \"rate\" 1\n",
      "\n",
      "('rate', 'rate')\n",
      "\"rate\" \"rate\" 1\n",
      "\n",
      "('rate', 'jege')\n",
      "\"rate\" \"jege\" 1\n",
      "\n",
      "('jege', 'jege')\n",
      "\"jege\" \"jege\" 1\n",
      "\n",
      "('jege', 'aschi')\n",
      "\"jege\" \"aschi\" 1\n",
      "\n",
      "('aschi', 'aschi')\n",
      "\"aschi\" \"aschi\" 1\n",
      "\n",
      "('aschi', 'ami')\n",
      "\"aschi\" \"ami\" 1\n",
      "\n",
      "('ami', 'ami')\n",
      "\"ami\" \"ami\" 1\n",
      "\n",
      "('ami', 'shudhu')\n",
      "\"ami\" \"shudhu\" 1\n",
      "\n",
      "('shudhu', 'shudhu')\n",
      "\"shudhu\" \"shudhu\" 1\n",
      "\n",
      "('shudhu', 'tOmake')\n",
      "\"shudhu\" \"tOmake\" 1\n",
      "\n",
      "('tOmake', 'Ekobar')\n",
      "\"tOmake\" \"Ekobar\" 1\n",
      "\n",
      "('Ekobar', 'chOcbob')\n",
      "\"Ekobar\" \"chOcbob\" 1\n",
      "\n",
      "('chOcbob', ',')\n",
      "\"chOcbob\" \"\" 1\n",
      "\n",
      "(',', 'taropor')\n",
      "\"\" \"taropor\" 1\n",
      "\n",
      "('taropor', 'hob')\n",
      "\"taropor\" \"hob\" 1\n",
      "\n",
      "('hob', 'itihasamar')\n",
      "\"hob\" \"itihasamar\" 1\n",
      "\n",
      "('itihasamar', 'shorIrer')\n",
      "\"itihasamar\" \"shorIrer\" 1\n",
      "\n",
      "('shorIrer', 'chaya')\n",
      "\"shorIrer\" \"chaya\" 1\n",
      "\n",
      "('chaya', 'ghurote')\n",
      "\"chaya\" \"ghurote\" 1\n",
      "\n",
      "('ghurote', 'ghurote')\n",
      "\"ghurote\" \"ghurote\" 1\n",
      "\n",
      "('ghurote', 'chayahIn')\n",
      "\"ghurote\" \"chayahIn\" 1\n",
      "\n",
      "('chayahIn', 'EkoTi')\n",
      "\"chayahIn\" \"EkoTi\" 1\n",
      "\n",
      "('EkoTi', 'rekhay')\n",
      "\"EkoTi\" \"rekhay\" 1\n",
      "\n",
      "('rekhay', 'Ese')\n",
      "\"rekhay\" \"Ese\" 1\n",
      "\n",
      "('Ese', 'dacboRiyechepagolI')\n",
      "\"Ese\" \"dacboRiyechepagolI\" 1\n",
      "\n",
      "('dacboRiyechepagolI', 'amar')\n",
      "\"dacboRiyechepagolI\" \"amar\" 1\n",
      "\n",
      "('amar', 'ghumiye')\n",
      "\"amar\" \"ghumiye\" 1\n",
      "\n",
      "('ghumiye', 'poReche')\n",
      "\"ghumiye\" \"poReche\" 1\n",
      "\n",
      "('poReche', 'muThOfOn')\n",
      "\"poReche\" \"muThOfOn\" 1\n",
      "\n",
      "('muThOfOn', 'tai')\n",
      "\"muThOfOn\" \"tai\" 1\n",
      "\n",
      "('tai', 'shant')\n",
      "\"tai\" \"shant\" 1\n",
      "\n",
      "('shant', ',')\n",
      "\"shant\" \"\" 1\n",
      "\n",
      "(',', 'ami')\n",
      "\"\" \"ami\" 1\n",
      "\n",
      "('ami', 'rat')\n",
      "\"ami\" \"rat\" 1\n",
      "\n",
      "('rat', 'jege')\n",
      "\"rat\" \"jege\" 1\n",
      "\n",
      "('jege', 'diCchi')\n",
      "\"jege\" \"diCchi\" 1\n",
      "\n",
      "('diCchi', 'pahara')\n",
      "\"diCchi\" \"pahara\" 1\n",
      "\n",
      "('pahara', 'muThOfOner')\n",
      "\"pahara\" \"muThOfOner\" 1\n",
      "\n",
      "('muThOfOner', 'Ei')\n",
      "\"muThOfOner\" \"Ei\" 1\n",
      "\n",
      "('Ei', 'prantamake')\n",
      "\"Ei\" \"prantamake\" 1\n",
      "\n",
      "('prantamake', 'diyeche')\n",
      "\"prantamake\" \"diyeche\" 1\n",
      "\n",
      "('diyeche', 'jonm')\n",
      "\"diyeche\" \"jonm\" 1\n",
      "\n",
      "('jonm', 'roktojhora')\n",
      "\"jonm\" \"roktojhora\" 1\n",
      "\n",
      "('roktojhora', 'omor')\n",
      "\"roktojhora\" \"omor\" 1\n",
      "\n",
      "('omor', 'Ekushena')\n",
      "\"omor\" \"Ekushena\" 1\n",
      "\n",
      "('Ekushena', 'jani')\n",
      "\"Ekushena\" \"jani\" 1\n",
      "\n",
      "('jani', 'se')\n",
      "\"jani\" \"se\" 1\n",
      "\n",
      "('se', 'aj')\n",
      "\"se\" \"aj\" 1\n",
      "\n",
      "('aj', 'kOn')\n",
      "\"aj\" \"kOn\" 1\n",
      "\n",
      "('kOn', 'rupe')\n",
      "\"kOn\" \"rupe\" 1\n",
      "\n",
      "('rupe', 'Eseche')\n",
      "\"rupe\" \"Eseche\" 1\n",
      "\n",
      "('Eseche', 'amar')\n",
      "\"Eseche\" \"amar\" 1\n",
      "\n",
      "('amar', 'gacboye')\n",
      "\"amar\" \"gacboye\" 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uni = open('unigram.txt', 'w')\n",
    "bi=open('bigram.txt', 'w')\n",
    "for i in range (len(a1)):\n",
    "    s = str(a1[i][0])\n",
    "    #rint(s)\n",
    "    s = s.replace('(', '')\n",
    "    s = s.replace(')', '')\n",
    "    s = s.replace(',', '')\n",
    "    #rint(s)\n",
    "    uni.write(s+\" \"+str(a1[i][1])+'\\n')\n",
    "for i in range (len(b1)):\n",
    "    s = str(b1[i][0])\n",
    "    print(s)\n",
    "    s = s.replace('(', '')\n",
    "    s = s.replace(')', '')\n",
    "    s = s.replace(',', '')\n",
    "    \n",
    "    s1 = s +\" \"+str(b1[i][1])+'\\n'\n",
    "    s1 = str(s1).replace(\"'\", '\"')\n",
    "    print(s1)\n",
    "    bi.write(s1)\n",
    "uni.close()\n",
    "bi.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9127570984652366e-07\n"
     ]
    }
   ],
   "source": [
    "text = \"chadni posor rate jege aschi ami chayahIn EkoTi rekhay Ese dacboRiyeche\n",
    "uni_list = list(Counter(Counter(ngrams(nltk.word_tokenize(text), 1))).items())\n",
    "bi_list = list(Counter(Counter(ngrams(nltk.word_tokenize(text), 2))).items())\n",
    "f1 = open('input.txt', 'w')\n",
    "for i in range (len(bi_list)):\n",
    "    s = str(bi_list[i][0])\n",
    "   # print(s)\n",
    "    s = s.replace('(', '')\n",
    "    s = s.replace(')', '')\n",
    "    s = s.replace(',', '')\n",
    "    f1.write(str(bi_list[i][0]) +\" \"+str(bi_list[i][1])+'\\n')\n",
    "f1.close()\n",
    "f = open('unigram.txt', 'r')\n",
    "ans = 1;\n",
    "dict_uni = {}\n",
    "for line in f.readlines():\n",
    "    line = line.split(\" \")\n",
    "   # print(line)\n",
    "    dict_uni[line[0]] = int(line[1])\n",
    "    #print(line[0])\n",
    "f = open('bigram.txt', 'r')\n",
    "dict_bi = {}\n",
    "for line in f.readlines():\n",
    "    #print(line)\n",
    "    line = line.split(' ')\n",
    "    s = str(line[0]+line[1])\n",
    "    #s = str(s).replace(\" \" \", ' ' )\n",
    "    dict_bi[s] = int(line[2])\n",
    "    #print(s)\n",
    "text = text.split(\" \")\n",
    "v = len(a)\n",
    "#print(dict_uni.keys())\n",
    "f1 = open('input.txt', 'r')\n",
    "for i in range (len(text)-1):\n",
    "    mul_up = 0\n",
    "    #print(\"#######################################\")\n",
    "    s = (\"\\\"\" + str(text[i])+ \"\\\"\"  + \"\\\"\"+str(text[i+1]) + \"\\\"\")\n",
    "    s1 = \"\\'\" + text[i]+ \"\\'\";\n",
    "    s2 = \"\\'\" + text[i+1]+ \"\\'\";\n",
    "    #print(dict_uni[s1])\n",
    "    if s in dict_bi.keys():\n",
    "        mul_up = dict_bi[s]\n",
    "        #print(\"comes\")\n",
    "    if s1 in dict_uni.keys():\n",
    "        mul_up = mul_up + dict_uni[s1]\n",
    "    if s2 in dict_uni.keys():\n",
    "        mul_up = mul_up + dict_uni[s2]\n",
    "    #mul_up = dict_bi[str(text[i])+ \" \" +str(text[i+1])] + dict_uni[text[i]]+dict_uni[text[i+1]]+1\n",
    "    mul_down =  v+total_words\n",
    "    #print(mul_up)\n",
    "    ans = ans* (mul_up/mul_down)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(text.split('')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
